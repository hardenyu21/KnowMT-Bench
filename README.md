# ğŸ“– KnowMT-Bench

Benchmarking Knowledge-Intensive Long Form Question Answering for LLMs in Multi-Turn Dialogues.

## ğŸš€ Get Started

```bash
# Create environment
conda create -n knowmt python=3.10
conda activate knowmt

# Install dependencies
pip install -r requirements.txt
```

## ğŸ› ï¸ Usage


### âš¡ï¸ Basic Evaluation
```bash
python eval.py --model qwen2.5-7b \
    --data_path data/KnowMT_QA.json \
    --output_path results/ \
```

### âš™ï¸ Arguments

Required:
- `--model`: Model name
- `--data_path`: Path to dataset
- `--output_path`: Output directory

Optional:
- `--mode {single,multi}`: Evaluation mode (default: multi)
- `--limit`: Max samples to evaluate
- `--resume_from_stage {1,2,3}`: Resume from stage

## ğŸ”„ Three-Stage Pipeline

1. **Generate**: Target model generates responses
2. **Decompose**: Extract factual statements
3. **Evaluate**: Assess factuality and hallucinations

Results saved in unified JSON format with metrics like `F1_fact`, `recall_fact`, etc.

## âœ… To Do


## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=hardenyu21/KnowMT-Bench&type=Date)](https://star-history.com/#hardenyu21/KnowMT-Bench)
